{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcd886-07f5-4a4b-a928-5187124b9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Formula 1 (F1) is the highest class of international auto racing for single-seater formula racing cars. It is widely regarded as the pinnacle of motorsport, featuring the fastest, most technologically advanced race cars and the best drivers in the world.\n",
    "\n",
    "#Question 1:\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# downloading stuff\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# original text\n",
    "para = \"Formula 1 (F1) is the highest class of international auto racing for single-seater formula racing cars. It is widely regarded as the pinnacle of motorsport, featuring the fastest, most technologically advanced race cars and the best drivers in the world.\n",
    "\"\n",
    "#Question 1:\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# downloading stuff\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# original text\n",
    "para = \"Formula 1 (F1) is the highest class of international auto racing for single-seater formula racing cars. It is widely regarded as the pinnacle of motorsport, featuring the fastest, most technologically advanced race cars and the best drivers in the world.\"\n",
    "\n",
    "# lowercase + remove punctuation\n",
    "lower_para = para.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# tokenize\n",
    "words = word_tokenize(lower_para)\n",
    "sentences = sent_tokenize(lower_para)\n",
    "\n",
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [w for w in words if w not in stop_words]\n",
    "\n",
    "# frequency\n",
    "fdist = FreqDist(filtered_words)\n",
    "\n",
    "print(\"Tokenized Sentences:\", sentences)\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "fdist.plot()\n",
    "\n",
    "#Question 2:\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stemming\n",
    "porter_stem = [porter.stem(word) for word in filtered_words]\n",
    "lancaster_stem = [lancaster.stem(word) for word in filtered_words]\n",
    "\n",
    "# lemmatization\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Original Words:\", filtered_words)\n",
    "print(\"Porter Stemmer:\", porter_stem)\n",
    "print(\"Lancaster Stemmer:\", lancaster_stem)\n",
    "print(\"Lemmatized Words:\", lemmas)\n",
    "\n",
    "\n",
    "#Quesiton 3:\n",
    "import re\n",
    "\n",
    "# a. Words with more than 5 letters\n",
    "big_words = re.findall(r'\\b\\w{6,}\\b', para)\n",
    "\n",
    "# b. Numbers\n",
    "numbers = re.findall(r'\\b\\d+\\b', para)\n",
    "\n",
    "# c. Capitalized words\n",
    "cap_words = re.findall(r'\\b[A-Z][a-z]*\\b', para)\n",
    "\n",
    "# splitting\n",
    "only_alpha = re.findall(r'\\b[a-zA-Z]+\\b', para)\n",
    "\n",
    "# words starting with vowel\n",
    "vowel_words = [w for w in only_alpha if w[0].lower() in 'aeiou']\n",
    "\n",
    "print(\"Words >5 letters:\", big_words)\n",
    "print(\"Numbers in text:\", numbers)\n",
    "print(\"Capitalized words:\", cap_words)\n",
    "print(\"Only alphabetic words:\", only_alpha)\n",
    "print(\"Words starting with vowel:\", vowel_words)\n",
    "\n",
    "\n",
    "#Question 4:\n",
    "# custom tokenizer\n",
    "def custom_tokenize(text):\n",
    "    # keep hyphens and apostrophes\n",
    "    text = re.sub(r'[^\\w\\s\\'\\-\\.]', '', text)\n",
    "    tokens = re.findall(r\"\\d+\\.\\d+|\\w+(?:-\\w+)*|'\\w+\", text)\n",
    "    return tokens\n",
    "\n",
    "tokens = custom_tokenize(para)\n",
    "\n",
    "# regex cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\b[\\w.-]+@[\\w.-]+\\.\\w{2,4}\\b', '<EMAIL>', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub(r'\\+?\\d[\\d\\s\\-]{8,}\\d', '<PHONE>', text)\n",
    "    return text\n",
    "\n",
    "cleaned = clean_text(para)\n",
    "\n",
    "print(\"Custom Tokens:\", tokens)\n",
    "print(\"Cleaned Text:\", cleaned)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
