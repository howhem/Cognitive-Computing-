{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fc34d-0e3e-4c19-ac2e-4d804af6c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Formula 1 (F1) is the highest class of international auto racing for single-seater formula racing cars. It is widely regarded as the pinnacle of motorsport, featuring the fastest, most technologically advanced race cars and the best drivers in the world\n",
    "\n",
    "#Question 1:\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# original text\n",
    "text = \"\"\"Formula 1 (F1) is the highest class of international auto racing for single-seater formula racing cars. It is widely regarded as the pinnacle of motorsport, featuring the fastest, most technologically advanced race cars and the best drivers in the world\"\"\"\n",
    "# 1. Lowercase + remove punctuation using re\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "# 2. Tokenize into words and sentences\n",
    "words = word_tokenize(clean_text)\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# 3. Compare split() vs word_tokenize()\n",
    "split_words = clean_text.split()\n",
    "word_tokenized = word_tokenize(clean_text)\n",
    "\n",
    "print(\"Using split():\", split_words[:10])\n",
    "print(\"Using word_tokenize():\", word_tokenized[:10])\n",
    "\n",
    "# 4. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in words if word not in stop_words]\n",
    "\n",
    "# 5. Word frequency distribution\n",
    "fdist = FreqDist(filtered)\n",
    "fdist.plot()\n",
    "\n",
    "#Question 2:\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 1. Extract words with only alphabets\n",
    "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "\n",
    "# 2. Remove stopwords\n",
    "filtered_alpha = [w for w in alpha_words if w not in stop_words]\n",
    "\n",
    "# 3. Stemming\n",
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(w) for w in filtered_alpha]\n",
    "\n",
    "# 4. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in filtered_alpha]\n",
    "\n",
    "# 5. Compare outputs\n",
    "print(\"Stemmed:\", stems[:20])\n",
    "print(\"Lemmatized:\", lemmas[:20])\n",
    "\n",
    "# Stemming is faster but can distort words (\"playing\" → \"play\", \"strategies\" → \"strategi\")\n",
    "# Lemmatization keeps proper words and grammar (\"strategies\" → \"strategy\"), better for readable output\n",
    "\n",
    "#Question 3:\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "texts = [\n",
    "    \"This phone has a great battery life.\",\n",
    "    \"The camera quality is not that good.\",\n",
    "    \"Amazing design and excellent performance.\"\n",
    "]\n",
    "\n",
    "# 1. CountVectorizer (BoW)\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(texts)\n",
    "print(\"BoW Features:\", cv.get_feature_names_out())\n",
    "print(bow.toarray())\n",
    "\n",
    "# 2. TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(texts)\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# 3. Top 3 keywords per text\n",
    "import numpy as np\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "for i, row in enumerate(tfidf_matrix.toarray()):\n",
    "    top_indices = row.argsort()[-3:][::-1]\n",
    "    top_keywords = [(feature_names[j], row[j]) for j in top_indices]\n",
    "    print(f\"Text {i+1} Top Keywords:\", top_keywords)\n",
    "\n",
    "#Question 4:\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "text1 = \"\"\"Artificial Intelligence is used in many areas like healthcare, robotics, and data analysis. It helps machines make decisions and learn from experience.\"\"\"\n",
    "text2 = \"\"\"Blockchain is a secure technology that records transactions in blocks. It is mostly used in cryptocurrencies and digital records.\"\"\"\n",
    "\n",
    "# Tokenization & preprocessing\n",
    "tokens1 = set(re.findall(r'\\b\\w+\\b', text1.lower()))\n",
    "tokens2 = set(re.findall(r'\\b\\w+\\b', text2.lower()))\n",
    "\n",
    "# a. Jaccard Similarity\n",
    "jaccard = len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
    "print(\"Jaccard Similarity:\", jaccard)\n",
    "\n",
    "# b. Cosine Similarity using TF-IDF\n",
    "vec = TfidfVectorizer()\n",
    "tfidf_matrix = vec.fit_transform([text1, text2])\n",
    "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "print(\"Cosine Similarity:\", cos_sim[0][0])\n",
    "\n",
    "# c. Insight\n",
    "# Cosine is better for longer texts because it considers word importance (TF-IDF)\n",
    "# Jaccard is simpler and works better for short, distinct texts\n",
    "\n",
    "#Question 5:\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reviews = [\n",
    "    \"The app is really useful and works smoothly.\",\n",
    "    \"Worst update ever, full of bugs!\",\n",
    "    \"It's okay, nothing special but not bad.\",\n",
    "    \"I love how fast and simple the UI is!\",\n",
    "    \"Terrible experience. Would not recommend.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    blob = TextBlob(review)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < -0.1:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    print(f\"Review: {review}\\nPolarity: {polarity:.2f} → {sentiment}\\n\")\n",
    "\n",
    "# Word Cloud for positive reviews\n",
    "positive_text = \" \".join([r for r in reviews if TextBlob(r).sentiment.polarity > 0.1])\n",
    "wordcloud = WordCloud(width=600, height=400, background_color='white').generate(positive_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Positive Review Word Cloud\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Question 6:\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Training text\n",
    "data = \"\"\"Esports is growing rapidly around the world. Players train hard to compete at high levels. Tournaments are watched by millions online. Careers in esports include players, coaches, and streamers. Teams often have sponsors and big fanbases.\"\"\"\n",
    "\n",
    "# 1. Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 2. Create sequences\n",
    "input_sequences = []\n",
    "tokens = tokenizer.texts_to_sequences([data])[0]\n",
    "for i in range(2, len(tokens)):\n",
    "    input_sequences.append(tokens[:i])\n",
    "\n",
    "# Pad sequences\n",
    "input_sequences = pad_sequences(input_sequences)\n",
    "\n",
    "# Split input and labels\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "y = np.array(y)\n",
    "\n",
    "# 3. Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=X.shape[1]))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"Esports is\"\n",
    "next_words = 5\n",
    "for _ in range(next_words):\n",
    "    token_seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_seq = pad_sequences([token_seq], maxlen=X.shape[1])\n",
    "    predicted = model.predict(token_seq, verbose=0).argmax()\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            seed_text += \" \" + word\n",
    "            break\n",
    "\n",
    "print(\"Generated Text:\", seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
